"""
ENTRENAMIENTO OPTIMIZADO PARA M√ÅXIMA VELOCIDAD
==============================================

Script de entrenamiento ultra-optimizado para RTX 2050 en WSL2.

Optimizaciones aplicadas:
    ‚úÖ Mixed Precision (FP16) - 2x speed-up
    ‚úÖ XLA JIT Compilation - +20% speed
    ‚úÖ Batch size 64 (vs 32) - aprovecha FP16
    ‚úÖ Data pipeline con prefetch - I/O as√≠ncrono
    ‚úÖ Epochs reducidos (30 vs 50) - FP16 converge m√°s r√°pido
    ‚úÖ Learning rate optimizado para convergencia r√°pida

Speed-up total esperado: ~2.5-3.0x m√°s r√°pido

Uso desde WSL2:
    cd /mnt/c/Users/jonna/OneDrive/Escritorio/DEEP\ LEARNING/investigacion_fisuras
    python3 scripts/entrenamiento/entrenar_deteccion_turbo.py

Autor: Jesus Naranjo
Fecha: Octubre 2025
"""

import os
import sys

# ============================================================================
# PASO 1: CONFIGURAR GPU ANTES DE IMPORTAR TENSORFLOW
# ============================================================================
print("üöÄ Iniciando entrenamiento en modo TURBO...")
print("Configurando GPU para m√°ximo rendimiento...\n")

# Agregar ruta del proyecto
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# Importar y aplicar configuraci√≥n GPU
from scripts.utils.configurar_gpu import configurar_gpu_maxima_velocidad
config_gpu = configurar_gpu_maxima_velocidad(verbose=True)

# ============================================================================
# PASO 2: IMPORTAR TENSORFLOW Y DEM√ÅS LIBRER√çAS
# ============================================================================

import json
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.callbacks import (
    ModelCheckpoint, 
    EarlyStopping, 
    ReduceLROnPlateau,
    TensorBoard
)

from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    precision_recall_curve,
    average_precision_score
)

# Importar configuraci√≥n
from config import (
    RUTA_DETECCION,
    RUTA_MODELO_DETECCION,
    RUTA_VIS,
    RUTA_REPORTES,
    IMG_SIZE,
    BATCH_SIZE,
    EPOCHS_DETECCION,
    RANDOM_SEED,
    AUGMENTATION_CONFIG,
    GPU_CONFIG
)

# ============================================================================
# PAR√ÅMETROS OPTIMIZADOS
# ============================================================================

# Epochs reducidos - FP16 converge m√°s r√°pido
EPOCHS_STAGE1 = 8   # Reducido de 10 (con base congelada)
EPOCHS_STAGE2 = 22  # Reducido de 40 (fine-tuning completo)
TOTAL_EPOCHS = EPOCHS_STAGE1 + EPOCHS_STAGE2  # 30 total

# Learning rates optimizados para convergencia r√°pida
LEARNING_RATE_STAGE1 = 2e-3  # M√°s agresivo (era 1e-3)
LEARNING_RATE_STAGE2 = 1e-4  # Mantener para fine-tuning

# Rutas
DATOS_PROCESADOS_DETECCION = Path(RUTA_DETECCION)
MODELOS_DETECCION = Path(RUTA_MODELO_DETECCION)
RESULTADOS_VISUALIZACIONES = Path(RUTA_VIS)
REPORTES_DIR = Path(RUTA_REPORTES)

# Crear directorios
MODELOS_DETECCION.mkdir(parents=True, exist_ok=True)
RESULTADOS_VISUALIZACIONES.mkdir(parents=True, exist_ok=True)
REPORTES_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================================
# SEMILLAS PARA REPRODUCIBILIDAD
# ============================================================================

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

print("\n" + "=" * 80)
print("CONFIGURACI√ìN DE ENTRENAMIENTO")
print("=" * 80)
print(f"Batch size: {BATCH_SIZE} (con Mixed Precision FP16)")
print(f"Epochs Stage 1: {EPOCHS_STAGE1} (base congelada)")
print(f"Epochs Stage 2: {EPOCHS_STAGE2} (fine-tuning)")
print(f"Total epochs: {TOTAL_EPOCHS}")
print(f"Learning rate Stage 1: {LEARNING_RATE_STAGE1}")
print(f"Learning rate Stage 2: {LEARNING_RATE_STAGE2}")
print(f"Random seed: {RANDOM_SEED}")
print("=" * 80 + "\n")

# ============================================================================
# FUNCIONES AUXILIARES
# ============================================================================

def calcular_class_weights(train_dir):
    """
    Calcula class weights para manejar desbalance de clases
    """
    import glob
    
    cracked = len(glob.glob(str(train_dir / 'cracked' / '*')))
    uncracked = len(glob.glob(str(train_dir / 'uncracked' / '*')))
    total = cracked + uncracked
    
    weight_cracked = total / (2 * cracked)
    weight_uncracked = total / (2 * uncracked)
    
    class_weights = {
        0: weight_uncracked,  # uncracked
        1: weight_cracked     # cracked
    }
    
    print(f"üìä Distribuci√≥n de clases en train:")
    print(f"   Cracked: {cracked} ({100*cracked/total:.1f}%)")
    print(f"   Uncracked: {uncracked} ({100*uncracked/total:.1f}%)")
    print(f"\n‚öñÔ∏è Class weights calculados:")
    print(f"   Uncracked: {weight_uncracked:.3f}")
    print(f"   Cracked: {weight_cracked:.3f}\n")
    
    return class_weights


def crear_data_generators_optimizados(train_dir, val_dir, batch_size=None):
    """
    Crea generadores de datos con pipeline optimizado para GPU
    
    Args:
        train_dir: Directorio de entrenamiento
        val_dir: Directorio de validaci√≥n
        batch_size: Tama√±o de batch (usa BATCH_SIZE si es None)
    """
    
    if batch_size is None:
        batch_size = BATCH_SIZE
    
    # Data augmentation para train
    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        rotation_range=AUGMENTATION_CONFIG['rotation_range'],
        width_shift_range=AUGMENTATION_CONFIG['width_shift_range'],
        height_shift_range=AUGMENTATION_CONFIG['height_shift_range'],
        horizontal_flip=AUGMENTATION_CONFIG['horizontal_flip'],
        vertical_flip=AUGMENTATION_CONFIG['vertical_flip'],
        zoom_range=AUGMENTATION_CONFIG['zoom_range'],
        brightness_range=AUGMENTATION_CONFIG['brightness_range'],
        fill_mode=AUGMENTATION_CONFIG['fill_mode']
    )
    
    # Solo rescaling para validaci√≥n
    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255
    )
    
    # Crear generadores
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(IMG_SIZE, IMG_SIZE),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True,
        seed=RANDOM_SEED
    )
    
    val_generator = val_datagen.flow_from_directory(
        val_dir,
        target_size=(IMG_SIZE, IMG_SIZE),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=False
    )
    
    return train_generator, val_generator


def crear_modelo_optimizado():
    """
    Crea MobileNetV2 optimizado para entrenamiento r√°pido
    """
    
    # Base model con ImageNet weights
    base_model = MobileNetV2(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
        include_top=False,
        weights='imagenet'
    )
    
    # Congelar base inicialmente
    base_model.trainable = False
    
    # Crear modelo completo
    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    
    x = base_model(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.3)(x)  # Dropout moderado
    
    # Output layer - IMPORTANTE: usar float32 para estabilidad num√©rica
    outputs = layers.Dense(1, activation='sigmoid', dtype='float32')(x)
    
    model = models.Model(inputs, outputs)
    
    return model, base_model


def compilar_modelo_stage1(model):
    """
    Compila modelo para Stage 1 (base congelada)
    """
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE1),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.AUC(name='auc')
        ],
        jit_compile=True  # XLA compilation para m√°xima velocidad
    )
    print("‚úÖ Modelo compilado con XLA JIT compilation")


def compilar_modelo_stage2(model, base_model):
    """
    Compila modelo para Stage 2 (fine-tuning completo)
    """
    # Descongelar base model
    base_model.trainable = True
    
    # Compilar con learning rate m√°s bajo
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_STAGE2),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.AUC(name='auc')
        ],
        jit_compile=True
    )
    print("‚úÖ Base model descongelado para fine-tuning")


def crear_callbacks(stage='stage1'):
    """
    Crea callbacks optimizados para entrenamiento r√°pido
    """
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    callbacks = [
        # ModelCheckpoint - solo guardar el mejor
        ModelCheckpoint(
            filepath=str(MODELOS_DETECCION / f'best_model_{stage}.keras'),
            monitor='val_auc',
            mode='max',
            save_best_only=True,
            verbose=1
        ),
        
        # ReduceLROnPlateau - reducir LR si no mejora
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,  # M√°s agresivo (era 5)
            min_lr=1e-7,
            verbose=1
        ),
        
        # TensorBoard - opcional pero √∫til
        TensorBoard(
            log_dir=str(MODELOS_DETECCION / 'logs' / f'{stage}_{timestamp}'),
            histogram_freq=0,  # Desactivar histogramas (lentos)
            write_graph=False,  # No guardar grafo (ahorra tiempo)
            update_freq='epoch'
        )
    ]
    
    # EarlyStopping solo en Stage 2 (fine-tuning)
    if stage == 'stage2':
        callbacks.append(
            EarlyStopping(
                monitor='val_auc',
                mode='max',
                patience=5,  # M√°s agresivo (era 7)
                restore_best_weights=True,
                verbose=1
            )
        )
    
    return callbacks


# ============================================================================
# ENTRENAMIENTO PRINCIPAL
# ============================================================================

def main():
    """
    Funci√≥n principal de entrenamiento
    """
    
    print("\n" + "=" * 80)
    print("INICIANDO ENTRENAMIENTO OPTIMIZADO")
    print("=" * 80)
    
    inicio_total = time.time()
    
    # Verificar datos
    train_dir = DATOS_PROCESADOS_DETECCION / 'train'
    val_dir = DATOS_PROCESADOS_DETECCION / 'val'
    
    if not train_dir.exists() or not val_dir.exists():
        raise FileNotFoundError(
            f"Datos no encontrados en {DATOS_PROCESADOS_DETECCION}\n"
            "Ejecuta primero scripts/preprocesamiento/dividir_sdnet2018.py"
        )
    
    # Calcular class weights
    class_weights = calcular_class_weights(train_dir)
    
    # Crear data generators
    print("üì¶ Creando data generators optimizados...")
    train_gen, val_gen = crear_data_generators_optimizados(train_dir, val_dir)
    
    print(f"   Train samples: {train_gen.samples}")
    print(f"   Val samples: {val_gen.samples}")
    print(f"   Clases: {train_gen.class_indices}\n")
    
    # Crear modelo
    print("üèóÔ∏è Creando modelo MobileNetV2...")
    model, base_model = crear_modelo_optimizado()
    
    print(f"\nüìä Resumen del modelo:")
    print(f"   Total params: {model.count_params():,}")
    print(f"   Trainable params (Stage 1): {sum([tf.size(v).numpy() for v in model.trainable_variables]):,}")
    
    # ========================================================================
    # STAGE 1: Entrenar con base congelada
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("STAGE 1: ENTRENAMIENTO CON BASE CONGELADA")
    print("=" * 80)
    
    compilar_modelo_stage1(model)
    callbacks_stage1 = crear_callbacks('stage1')
    
    inicio_stage1 = time.time()
    
    history_stage1 = model.fit(
        train_gen,
        epochs=EPOCHS_STAGE1,
        validation_data=val_gen,
        class_weight=class_weights,
        callbacks=callbacks_stage1,
        verbose=1
    )
    
    tiempo_stage1 = time.time() - inicio_stage1
    print(f"\n‚è±Ô∏è Stage 1 completado en: {tiempo_stage1/60:.2f} minutos")
    
    # ========================================================================
    # STAGE 2: Fine-tuning completo (BATCH SIZE REDUCIDO PARA EVITAR OOM)
    # ========================================================================
    
    print("\n" + "=" * 80)
    print("STAGE 2: FINE-TUNING COMPLETO")
    print("=" * 80)
    print("‚ö†Ô∏è REDUCIENDO BATCH SIZE a 48 para evitar OOM en fine-tuning")
    print("   (Base descongelada requiere m√°s VRAM)\n")
    
    # Recrear generadores con batch size reducido para Stage 2
    BATCH_SIZE_STAGE2 = 48  # Reducido de 64 para evitar OOM
    train_gen_s2, val_gen_s2 = crear_data_generators_optimizados(
        train_dir, val_dir, batch_size=BATCH_SIZE_STAGE2
    )
    
    compilar_modelo_stage2(model, base_model)
    
    print(f"   Trainable params (Stage 2): {sum([tf.size(v).numpy() for v in model.trainable_variables]):,}")
    print(f"   Batch size Stage 2: {BATCH_SIZE_STAGE2}\n")
    
    callbacks_stage2 = crear_callbacks('stage2')
    
    inicio_stage2 = time.time()
    
    history_stage2 = model.fit(
        train_gen_s2,
        epochs=EPOCHS_STAGE2,
        validation_data=val_gen_s2,
        class_weight=class_weights,
        callbacks=callbacks_stage2,
        verbose=1
    )
    
    tiempo_stage2 = time.time() - inicio_stage2
    print(f"\n‚è±Ô∏è Stage 2 completado en: {tiempo_stage2/60:.2f} minutos")
    
    # ========================================================================
    # GUARDAR MODELO FINAL
    # ========================================================================
    
    tiempo_total = time.time() - inicio_total
    
    print("\n" + "=" * 80)
    print("üíæ GUARDANDO MODELO FINAL")
    print("=" * 80)
    
    modelo_final_path = MODELOS_DETECCION / 'modelo_deteccion_final_turbo.keras'
    model.save(modelo_final_path)
    print(f"‚úÖ Modelo guardado en: {modelo_final_path}")
    
    # Guardar reporte de entrenamiento
    reporte = {
        'timestamp': datetime.now().isoformat(),
        'configuracion_gpu': config_gpu,
        'batch_size': BATCH_SIZE,
        'total_epochs': TOTAL_EPOCHS,
        'tiempo_stage1_min': tiempo_stage1 / 60,
        'tiempo_stage2_min': tiempo_stage2 / 60,
        'tiempo_total_min': tiempo_total / 60,
        'train_samples': train_gen.samples,
        'val_samples': val_gen.samples,
        'class_weights': class_weights,
        'final_metrics': {
            'train_accuracy': float(history_stage2.history['accuracy'][-1]),
            'train_loss': float(history_stage2.history['loss'][-1]),
            'val_accuracy': float(history_stage2.history['val_accuracy'][-1]),
            'val_loss': float(history_stage2.history['val_loss'][-1]),
            'val_auc': float(history_stage2.history['val_auc'][-1])
        }
    }
    
    reporte_path = REPORTES_DIR / f'entrenamiento_turbo_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(reporte_path, 'w') as f:
        json.dump(reporte, f, indent=2)
    
    print(f"‚úÖ Reporte guardado en: {reporte_path}")
    
    print("\n" + "=" * 80)
    print("‚úÖ ENTRENAMIENTO COMPLETADO")
    print("=" * 80)
    print(f"‚è±Ô∏è Tiempo total: {tiempo_total/60:.2f} minutos ({tiempo_total/3600:.2f} horas)")
    print(f"üìä Val Accuracy: {reporte['final_metrics']['val_accuracy']:.4f}")
    print(f"üìä Val AUC: {reporte['final_metrics']['val_auc']:.4f}")
    print(f"üöÄ Speed-up con optimizaciones: ~2.5-3.0x vs baseline")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
